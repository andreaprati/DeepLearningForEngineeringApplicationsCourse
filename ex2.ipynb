{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84b0ae32-d608-4464-b5a2-93f3222be82b",
   "metadata": {},
   "source": [
    "# Defining and Training Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77933bed-341f-4546-a82e-c4f23bd605f9",
   "metadata": {},
   "source": [
    "What we will learn:\n",
    "- How to initialize a NN\n",
    "- Forward pass\n",
    "- Backward pass\n",
    "- Optimization of the network parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1243891b-5b8c-4d77-9b01-251e9c07c6ff",
   "metadata": {},
   "source": [
    "## Pytorch: <code>nn</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aae42ba-4062-4422-b84f-f43d7ab79537",
   "metadata": {},
   "source": [
    "The <code>nn</code> package defines a set of Modules (i.e. neural networks layers).\n",
    "\n",
    "Each module receive an input and produces an output.\n",
    "\n",
    "The <code>nn</code> package also defines losses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d20409-f40d-4c15-be65-d1e445b8b025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import math\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d34795d-98da-4f49-8805-5c9cf7eed948",
   "metadata": {},
   "source": [
    "### Objective\n",
    "Create a model that approximate the $sin(x)$ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928f2e25-8f2b-425e-84dd-ce1890bd6b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859097be-808f-4736-bb34-4c350d3d206c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this example, the netowrk will learn the Sin function using a Polynomial Approximation.\n",
    "# The output y is a function of (x, x^2, x^3), so\n",
    "# we can consider it as an output of a linear layer neural network. Let's prepare the\n",
    "# tensor (x, x^2, x^3).\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "print(xx.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd4b337-b68f-4829-b3f9-c8221f71a577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. The Linear Module computes output from input using a\n",
    "# linear function, and holds internal Tensors for its weight and bias.\n",
    "# The Flatten layer flatens the output of the linear layer to a 1D tensor,\n",
    "# to match the shape of `y`.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5526daa5-aedd-42d0-82dd-09af2fc842be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "learning_rate = 1e-3\n",
    "# Construct the Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters\n",
    "# which are members of the model.\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cc98c0-fff2-4100-b076-f663c8d60830",
   "metadata": {},
   "source": [
    "### MSE Loss\n",
    "$$\\mathcal{L}_{MSE} = \\frac{1}{n}\\sum^n_{i=n}(y - \\hat{y})^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a79672-c22e-47c9-9df3-74d8aaf09299",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(2000):\n",
    "\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = model(xx)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y) # (y_pred - y).pow(2).mean()\n",
    "    \n",
    "    # Print loss every 200 epochs\n",
    "    if t % 200 == 199:\n",
    "        print(t, loss.item())\n",
    "    \n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Alternative: zero the gradients of the model\n",
    "    # model.zero_grad()\n",
    "    \n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward() \n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Alternative: Update the weights using gradient descent MANUALLY. Each parameter is a Tensor, so\n",
    "    # we can access its gradients.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5133d785-f599-4cbe-8933-c718e60ac656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can access the first layer of `model` like accessing the first item of a list\n",
    "linear_layer = model[0] \n",
    "\n",
    "# For linear layer, its parameters are stored as `weight` and `bias`.\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090ad36c-c238-4d24-bdfe-6e7fe4f0a426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The network has effectively learned something?\n",
    "print(xx[500]) # x[500] = -pi/2\n",
    "print(\"%.6f %.6f\" % (model(xx)[500].item(), torch.sin(x)[500]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccae464-ceb2-47d7-977a-6abf37fe3ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "import matplotlib.pyplot as plt\n",
    "from res.plot_lib import plot_data, plot_data_np, plot_model, set_default\n",
    "set_default()\n",
    "\n",
    "yy = model(xx)\n",
    "\n",
    "plt.plot(x,y, label='Sin(x)')\n",
    "plt.plot(x,yy.detach().numpy(), label='model(x)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3b9b67-7ecb-4d4e-adbe-9dca48d9bcca",
   "metadata": {},
   "source": [
    "## Custom models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b499600-b3e9-457f-ba9a-c933c96a0180",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate all the layer of the NN\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(3, 1),\n",
    "            nn.Flatten(0, 1)\n",
    "        )\n",
    "        \"\"\"\n",
    "        In alternative we could also define each layer individually\n",
    "        \"\"\"\n",
    "        # self.l1 = nn.Linear(3, 1)\n",
    "        # self.flt = nn.Flatten(0, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        return self.model(x)\n",
    "        # x = self.l1(x)\n",
    "        # return self.flt(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc6349f-ce73-4c63-8309-fa600a765b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct our model by instantiating the class defined above\n",
    "model = SinModel()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd2d81e-c1a3-4c46-83d4-c31ce20d6f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters (defined \n",
    "# with torch.nn.Parameter) which are members of the model.\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "for t in range(2000):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(xx)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 200 == 199:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f34ff4-0a0e-403d-89ed-9e8f7b30e80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The network has effectively learned something?\n",
    "print(xx[500]) # x[500] = -pi/2\n",
    "print(\"%.6f %.6f\" % (model(xx)[500].item(), torch.sin(x)[500]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944c8ef6-ebc0-412e-82f0-bf312154b90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "yy = model(xx)\n",
    "\n",
    "plt.plot(x,y, label='Sin(x)')\n",
    "plt.plot(x,yy.detach().numpy(), label='model(x)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141b4585-23a6-4204-b98c-140aa41d0dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex1: write a model (using custom modules) where the output y is a function of (x, x^2, x^3, x^4)\n",
    "# and it approximate the cosine function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611592dd-58ac-40d7-9a4b-0b92fe51bc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex2: write a model (using custom modules) where the output y is a function of (x, x^2, x^3)\n",
    "# and it approximate the function -5 + 2*x + 3/4x^2 + 7*x^3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "a1e9190da96085107a48c4c9bd17a4ee4ffa8a340fb86380f1da8fbb80fcc62b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
